#!/usr/bin/env bash
set -euo pipefail

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”— AZURE DATA PIPELINE INTEGRATION SCRIPT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Links ADLS Gen2 storage to Databricks workspace for Scout Analytics
# Prerequisites: Azure CLI logged in, Databricks CLI configured
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

echo "ğŸ”— Starting Azure Data Pipeline Integration..."
echo "ğŸ“… Timestamp: $(date)"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CONFIGURATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RG="rg-scout-prod"
KV="kv-scout-prod"
DBW="dbx-scout-prod"
ST="stscoutprod"
CONTAINER="rawtransactions"

echo "ğŸ“‹ Configuration:"
echo "  Resource Group: $RG"
echo "  Key Vault: $KV"
echo "  Databricks: $DBW"
echo "  Storage: $ST"
echo "  Container: $CONTAINER"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FETCH SECRETS FROM KEY VAULT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo ""
echo "ğŸ” Fetching secrets from Azure Key Vault..."

SP=$(az keyvault secret show -n sp-client-id --vault-name $KV --query value -o tsv)
SP_KEY=$(az keyvault secret show -n sp-secret --vault-name $KV --query value -o tsv)
TENANT=$(az keyvault secret show -n sp-tenant --vault-name $KV --query value -o tsv)
STORAGE_KEY=$(az keyvault secret show -n storage-key --vault-name $KV --query value -o tsv)

echo "âœ… Retrieved service principal credentials"
echo "âœ… Retrieved storage account key"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DATABRICKS SECRET SCOPE SETUP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo ""
echo "ğŸ”’ Setting up Databricks secret scope..."

# Create secret scope (ignore if exists)
databricks secrets create-scope --scope scout 2>/dev/null || echo "Secret scope 'scout' already exists"

# Store secrets in Databricks
echo "ğŸ“ Storing secrets in Databricks scope..."
echo "$SP" | databricks secrets put --scope scout --key client_id
echo "$SP_KEY" | databricks secrets put --scope scout --key client_secret
echo "$TENANT" | databricks secrets put --scope scout --key tenant_id
echo "$STORAGE_KEY" | databricks secrets put --scope scout --key storage_key

echo "âœ… Databricks secrets configured"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ADLS MOUNT SCRIPT CREATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo ""
echo "ğŸ“¦ Creating ADLS mount script..."

cat > /tmp/mount_adls.py <<'PY'
# Databricks ADLS Gen2 Mount Script
# Mounts Scout raw transaction data from Azure Data Lake Storage

print("ğŸ”— Starting ADLS Gen2 mount process...")

# OAuth configuration for ADLS Gen2
configs = {
    "fs.azure.account.auth.type": "OAuth",
    "fs.azure.account.oauth.provider.type": 
        "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
    "fs.azure.account.oauth2.client.id": 
        dbutils.secrets.get(scope="scout", key="client_id"),
    "fs.azure.account.oauth2.client.secret": 
        dbutils.secrets.get(scope="scout", key="client_secret"),
    "fs.azure.account.oauth2.client.endpoint": 
        f"https://login.microsoftonline.com/{dbutils.secrets.get(scope='scout', key='tenant_id')}/oauth2/token"
}

# Mount ADLS container
try:
    # Check if already mounted
    mounts = dbutils.fs.mounts()
    mount_exists = any(mount.mountPoint == "/mnt/scout/raw" for mount in mounts)
    
    if mount_exists:
        print("ğŸ“ Mount point /mnt/scout/raw already exists")
        dbutils.fs.unmount("/mnt/scout/raw")
        print("ğŸ”„ Unmounted existing mount point")
    
    # Create new mount
    dbutils.fs.mount(
        source="abfss://rawtransactions@stscoutprod.dfs.core.windows.net/",
        mount_point="/mnt/scout/raw",
        extra_configs=configs
    )
    
    print("âœ… Successfully mounted ADLS Gen2 at /mnt/scout/raw")
    
    # Test mount by listing files
    files = dbutils.fs.ls("/mnt/scout/raw")
    print(f"ğŸ“‚ Found {len(files)} items in mounted directory")
    
    # Display first few files for verification
    for i, file in enumerate(files[:5]):
        print(f"  {i+1}. {file.name} ({file.size} bytes)")
    
except Exception as e:
    print(f"âŒ Mount failed: {str(e)}")
    raise e

print("ğŸ‰ ADLS Gen2 mount completed successfully!")
PY

# Upload mount script to DBFS
echo "ğŸ“¤ Uploading mount script to DBFS..."
databricks fs cp /tmp/mount_adls.py dbfs:/FileStore/scripts/mount_adls.py --overwrite

# Execute mount script
echo "ğŸš€ Executing mount script..."
databricks runs submit --python-file dbfs:/FileStore/scripts/mount_adls.py --run-name "scout-adls-mount-$(date +%Y%m%d-%H%M%S)"

echo "âœ… ADLS Gen2 mount script executed"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# DATABASE SETUP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo ""
echo "ğŸ—„ï¸  Setting up Databricks databases..."

cat > /tmp/setup_databases.py <<'PY'
# Create Scout databases and tables in Databricks

print("ğŸ—„ï¸  Creating Scout databases...")

# Create database
spark.sql("CREATE DATABASE IF NOT EXISTS scout")
spark.sql("USE scout")

print("âœ… Database 'scout' created/verified")

# Create Bronze table schema
spark.sql("""
CREATE TABLE IF NOT EXISTS scout.bronze_transactions (
    store_id INT,
    ts TIMESTAMP,
    sku STRING,
    qty INT,
    peso DOUBLE,
    request STRING,
    suggested BOOLEAN,
    customer_id INT,
    barangay STRING,
    gender STRING,
    age INT,
    ingest_time TIMESTAMP
) USING DELTA
""")

print("âœ… Bronze transactions table created")

# Create Silver table schema
spark.sql("""
CREATE TABLE IF NOT EXISTS scout.silver_transactions (
    store_id INT,
    ts TIMESTAMP,
    date DATE,
    sku STRING,
    qty INT,
    peso DOUBLE,
    request STRING,
    suggested BOOLEAN,
    customer_id INT,
    barangay STRING,
    gender STRING,
    age INT,
    ingest_time TIMESTAMP
) USING DELTA
""")

print("âœ… Silver transactions table created")

# Create Gold KPIs table schema
spark.sql("""
CREATE TABLE IF NOT EXISTS scout.gold_daily_kpis (
    date DATE,
    revenues DOUBLE,
    orders BIGINT,
    aov DOUBLE
) USING DELTA
""")

print("âœ… Gold daily KPIs table created")

print("ğŸ‰ Database setup completed!")
PY

# Upload and execute database setup
echo "ğŸ“¤ Uploading database setup script..."
databricks fs cp /tmp/setup_databases.py dbfs:/FileStore/scripts/setup_databases.py --overwrite

echo "ğŸš€ Executing database setup..."
databricks runs submit --python-file dbfs:/FileStore/scripts/setup_databases.py --run-name "scout-db-setup-$(date +%Y%m%d-%H%M%S)"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# VERIFICATION
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo ""
echo "ğŸ” Verifying setup..."

# Test mount point
echo "ğŸ“ Testing mount point access..."
databricks fs ls dbfs:/mnt/scout/raw/ || echo "âš ï¸  Mount verification failed - check logs"

# Test database access
echo "ğŸ—„ï¸  Testing database access..."
databricks runs submit --python-file dbfs:/FileStore/scripts/setup_databases.py --run-name "scout-verify-$(date +%Y%m%d-%H%M%S)"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLEANUP
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo ""
echo "ğŸ§¹ Cleaning up temporary files..."
rm -f /tmp/mount_adls.py /tmp/setup_databases.py

echo ""
echo "ğŸ‰ AZURE DATA PIPELINE INTEGRATION COMPLETED!"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“‹ SUMMARY:"
echo "  âœ… Databricks secret scope 'scout' created"
echo "  âœ… Service principal credentials stored"
echo "  âœ… ADLS Gen2 mounted at /mnt/scout/raw"
echo "  âœ… Scout database and tables created"
echo "  âœ… Ready for ETL job deployment"
echo ""
echo "ğŸš€ NEXT STEPS:"
echo "  1. Deploy ETL jobs using CI/CD pipeline"
echo "  2. Configure Azure OpenAI integration"
echo "  3. Test end-to-end data flow"
echo "  4. Monitor job execution and data quality"
echo ""
echo "ğŸ“Š Your Azure data pipeline is ready for Scout Analytics!"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
